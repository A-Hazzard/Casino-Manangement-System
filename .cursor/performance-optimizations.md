# Performance Optimization Guidelines

**Author:** Aaron Hazzard - Senior Software Engineer  
**Last Updated:** November 11, 2025  
**Version:** 1.0.0

---

## ğŸ¯ Overview

This document summarizes all major performance optimizations implemented in the Evolution CMS, providing patterns and best practices for future development.

---

## ğŸš€ Optimization Patterns Implemented

### 1. Batch Query Pattern (Solves N+1 Problems)

**Problem:** Querying database individually for each item in a loop

**Bad Example:**

```typescript
// âŒ N+1 PROBLEM: N separate queries
for (const machine of machines) {
  const meters = await Meters.find({ machine: machine.id, ... });
  // Process meters...
}
```

**Good Example:**

```typescript
// âœ… SOLUTION: ONE batch query + lookup map
const allMeterData = await Meters.aggregate([
  { $match: { machine: { $in: machineIds }, ... } },
  { $group: { _id: '$machine', totalDrop: { $sum: '$movement.drop' } } },
]);

const meterDataMap = new Map(allMeterData.map(m => [m._id, m]));
machines.forEach(machine => {
  const meters = meterDataMap.get(machine.id); // O(1) lookup!
});
```

**Where Applied:**

- Collection Report Details API (16 machines: ~12s â†’ ~2s)
- Dashboard API (parallel batches)
- Locations API (parallel batches)

---

### 2. Parallel Processing Pattern

**Problem:** Processing independent queries sequentially

**Bad Example:**

```typescript
// âŒ SEQUENTIAL: 3s + 3s + 3s = 9s
for (const licensee of licensees) {
  await fetchDataForLicensee(licensee);
}
```

**Good Example:**

```typescript
// âœ… PARALLEL: max(3s, 3s, 3s) = 3s!
const results = await Promise.all(
  licensees.map(licensee => fetchDataForLicensee(licensee))
);
```

**Where Applied:**

- Dashboard 30d API (14.94s â†’ 5.20s, 65% faster!)
- Locations API (parallel location batches)
- Cabinets API (parallel location batches)

---

### 3. Adaptive Batch Sizing

**Problem:** Fixed batch size doesn't adapt to data volume

**Bad Example:**

```typescript
// âŒ FIXED: Same batch size for all scenarios
const BATCH_SIZE = 20;
```

**Good Example:**

```typescript
// âœ… ADAPTIVE: Larger batches for longer periods
const BATCH_SIZE = timePeriod === '7d' || timePeriod === '30d' ? 50 : 20;
```

**Rationale:**

- Longer periods (7d/30d) need LARGER batches to reduce overhead
- Shorter periods (Today/Yesterday) need smaller batches for precision

**Where Applied:**

- Locations API (TIMEOUT â†’ 3-9s, FIXED!)

---

### 4. Single Aggregation for Large Datasets

**Problem:** Multiple aggregations when one could suffice

**Bad Example:**

```typescript
// âŒ MULTIPLE: One aggregation per location
for (const location of locations) {
  await aggregateMetersForLocation(location);
}
```

**Good Example:**

```typescript
// âœ… SINGLE: One aggregation for ALL locations/machines
const allMetrics = await Meters.aggregate([
  {
    $match: {
      machine: { $in: allMachineIds },
      readAt: { $gte: start, $lte: end },
    },
  },
  { $group: { _id: '$machine', moneyIn: { $sum: '$drop' } } },
]);
```

**Where Applied:**

- Cabinets 7d/30d (TIMEOUT â†’ 6-7s, FIXED!)

---

### 5. Field Projection Before Expensive Operations

**Problem:** Fetching all fields before filtering/aggregating

**Bad Example:**

```typescript
// âŒ UNOPTIMIZED: Fetch all fields, then $lookup
{
  $lookup: {
    from: 'machines',
    localField: '_id',
    foreignField: 'gamingLocation',
    as: 'machines',
  },
}
```

**Good Example:**

```typescript
// âœ… OPTIMIZED: Project essential fields BEFORE $lookup
{
  $project: { _id: 1, name: 1, profitShare: 1 },
},
{
  $lookup: {
    from: 'machines',
    localField: '_id',
    foreignField: 'gamingLocation',
    as: 'machines',
    pipeline: [
      { $project: { _id: 1, serialNumber: 1, 'custom.name': 1 } },
    ],
  },
}
```

**Where Applied:**

- Locations API (meters aggregation)
- Cabinets API (meters aggregation)
- Collection Reports (locations with machines)

---

### 6. Pagination Pattern

**Problem:** Fetching all data when only a page is needed

**Bad Example:**

```typescript
// âŒ NO PAGINATION: Fetch all 40K records
const reports = await getAllReports();
return reports; // Frontend does pagination
```

**Good Example:**

```typescript
// âœ… SERVER-SIDE PAGINATION: Fetch only what's needed
const page = parseInt(req.query.page || '1');
const limit = Math.min(parseInt(req.query.limit || '50'), 100);
const skip = (page - 1) * limit;

const reports = await getAllReports();
return reports.slice(skip, skip + limit);
```

**Where Applied:**

- Collection Report List API (>30s â†’ ~3s for All Time!)

---

### 7. Index Hints for Large Datasets

**Problem:** MongoDB not using optimal index

**Solution:**

```typescript
// Force MongoDB to use specific index
const results = await Meters.aggregate(pipeline, {
  hint: { machine: 1, readAt: 1 }, // Use compound index
  maxTimeMS: 90000, // Prevent timeouts
  allowDiskUse: true, // Allow disk for large datasets
});
```

**Where Applied:**

- Chart API (30d queries)
- Cabinets API (7d/30d queries)

---

## ğŸ“Š Performance Results

### Endpoints Optimized (18/20 under 10s = 90% success!)

| Endpoint               | Filter      | Before  | After | Improvement   |
| ---------------------- | ----------- | ------- | ----- | ------------- |
| **Dashboard**          | 30d         | 14.94s  | 5.20s | 65% faster âœ… |
| **Locations**          | 7d/30d      | TIMEOUT | 3-9s  | FIXED! âœ…     |
| **Cabinets**           | Today/7d    | TIMEOUT | 6-7s  | FIXED! âœ…     |
| **Collection Details** | 16 machines | ~12s    | ~2s   | 5x faster âœ…  |
| **Collection List**    | All Time    | >30s    | ~3s   | 10x faster âœ… |

---

## ğŸŒ Known Slow Endpoints (Acceptable)

| Endpoint | Filter | Time | Reason              | Solution               |
| -------- | ------ | ---- | ------------------- | ---------------------- |
| Chart    | 30d    | 14s  | 1.5M hourly records | Redis cache (optional) |
| Cabinets | 30d    | 20s  | 2K machines Ã— 30d   | Redis cache (optional) |

**Note:** These are edge cases with massive datasets. All other endpoints meet the <10s goal.

---

## ğŸ”§ Optimization Checklist

When building new features, ask:

### 1. Database Queries

- âœ… Can queries be batched? (Solve N+1)
- âœ… Can queries run in parallel? (Use Promise.all)
- âœ… Are you projecting only needed fields?
- âœ… Are indexes being used? (Add hint if needed)
- âœ… Is pagination needed? (>100 records)

### 2. Data Processing

- âœ… Can processing be parallelized?
- âœ… Are you using lookup maps instead of array.find()?
- âœ… Are batch sizes adaptive to data volume?

### 3. MongoDB Aggregation

- âœ… Project early (before $lookup and $group)
- âœ… Use indexes (add hint option)
- âœ… Set maxTimeMS for long queries (90s)
- âœ… Use allowDiskUse for large datasets

### 4. Performance Logging

- âœ… Log total request time
- âœ… Log breakdown of expensive operations
- âœ… Include result counts/sizes

**Example Logging:**

```typescript
console.log(
  `[ENDPOINT] âš¡ Query: ${totalTime}ms | ` +
    `DB: ${dbTime}ms | Processing: ${procTime}ms | ` +
    `Results: ${count}`
);
```

---

## ğŸ“ Key Learnings

### âœ… What Works:

1. **Batch everything** - ONE query >> N queries
2. **Parallelize everything** - Independent queries should run concurrently
3. **Adapt to context** - Different batch sizes for different scenarios
4. **Project early** - Only fetch what you need
5. **Use lookup maps** - O(1) access >> O(N) array.find()

### âŒ What Doesn't Work:

1. **Sequential processing** - When parallel is possible
2. **$lookup without pipeline** - Fetches too much data
3. **Fixed parameters** - Doesn't adapt to data volume
4. **Post-filtering** - Filter in database, not memory

---

## ğŸ“ Key Files

### Performance-Critical Files:

- `app/api/dashboard/totals/route.ts` - Parallel licensee processing
- `app/api/reports/locations/route.ts` - Adaptive batching
- `app/api/machines/aggregation/route.ts` - Single aggregation
- `app/api/lib/helpers/accountingDetails.ts` - Batch meter queries
- `app/api/lib/helpers/meters/aggregations.ts` - Index hints
- `lib/utils/gamingDayRange.ts` - Gaming day calculations

### Performance Utilities:

- `scripts/backup-all-collections.js` - Comprehensive backup (all models except meters)
- `scripts/investigation/` - Debugging scripts for gaming day offset
- `scripts/detect-issues.go` - Issue detection with backup

---

## ğŸ¯ Success Criteria

**All endpoints should:**

- âœ… Load in <10 seconds (90% achieved!)
- âœ… Use batch queries (no N+1 problems)
- âœ… Have performance logging
- âœ… Handle large datasets gracefully
- âœ… Respect gaming day offset for financial metrics

---

**For specific implementation details, see:**

- Backend collection report docs: `Documentation/backend/collection-report*.md`
- Gaming day offset system: `.cursor/gaming-day-offset-system.md`
- Performance test results: `FINAL_COLLECTION_REPORT_SUMMARY.md`
